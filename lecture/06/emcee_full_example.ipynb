{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR 496: An Emcee Worked example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import corner\n",
    "import astropy.table as at\n",
    "import scipy.optimize as so\n",
    "import pandas as pd\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Emcee for a real problem where a grid would have been really painful.\n",
    "\n",
    "\n",
    "We'll be analyzing data from the Optical Gravitational Lensing Experiment (OGLE), which monitors stars in our galaxy in the hopes of detecting gravitational microlensing events that occur when a compact mass (e.g. a fainter star) passes in front of the monitored star.\n",
    "\n",
    "You can read more about microlensing here if you like:\n",
    "https://en.wikipedia.org/wiki/Gravitational_microlensing\n",
    "\n",
    "Data are available through the [OGLE Early Warning System](http://ogle.astrouw.edu.pl/ogle4/ews/ews.html). I've chosen this event at random:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"http://ogle.astrouw.edu.pl/ogle4/ews/2019/data/2019/blg-0001/lcurve.gif\" width=75%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "You can pick your own if you like. As long as a vaguely reasonable looking magenta line is shown on the OGLE page, this should be a good data set to fit. Download the `phot.dat` for your chosen event (linked at the bottom of the webpage).\n",
    "\n",
    "As described on the OGLE page, the columns of this text file are\n",
    "\n",
    "> Hel.JD, I magnitude, magnitude error, seeing estimation (in pixels - 0.26\"/pixel) and sky level\n",
    "\n",
    "* Heliocentric Julian Date. This is time, measured in days, since a fixed reference. The \"heliocentric\" part means that it has been corrected to the reference frame of the Sun, i.e. the few minutes of light travel time more or less that would affect photon arrivals at different parts of the Earth's year have been subtracted off.\n",
    "\n",
    "* Measurements of magnitude in the $I$ band (a near infrared band). Recall that astronomical magnitude, relative to a given reference source, is given by the relationship $m = m_\\mathrm{ref} - 2.5\\,\\log_{10}\\left(\\frac{F}{F_\\mathrm{ref}}\\right)$, where $F$ is flux.\n",
    "\n",
    "* Measurement uncertainty on the $I$ magnitude, defined in some unspecified way (digging through papers might elucidate this).\n",
    "\n",
    "* The \"seeing\" and \"sky level\" quantities refer to the observing conditions, which we will not work with directly. These will have been accounted for (somehow) in deriving the best-fitting magnitude and its uncertainty.\n",
    "\n",
    "\n",
    "# As with your homework 03 (i.e. this is as big a hint as you get from me):\n",
    "\n",
    "\n",
    "We have 4 questions to answer\n",
    "1. What's the model?\n",
    "2. What's the Likelihood?\n",
    "3. What's the Prior?\n",
    "4. How do you sample?\n",
    "\n",
    "\n",
    "In this case 1 is a known model to describe Microlensing called the [Paczyinski model](https://en.wikipedia.org/wiki/Bohdan_Paczy%C5%84ski). \n",
    "\n",
    "## $$F(t) = F_0 \\frac{u(t)^2 + 2}{u(t)\\sqrt{u(t)^2+4}}$$\n",
    "\n",
    "where\n",
    "\n",
    "## $$u(t) = \\sqrt{p^2 + \\left( \\frac{t-t_\\mathrm{max}}{t_\\mathrm{E}} \\right)^2}$$\n",
    "\n",
    "You'll of course also need the transformation between flux and magnitude, above. For convenience, let's parameterize the normalization of the model lightcurve in magnitudes rather than flux, i.e. $I_0$ rather than $F_0$; that way, all of the \"ref\" quantities in the magnitude definition are absorbed into this new parameter and we won't have to worry about them explicitly. With that substitution, the model parameters are $I_0$, $p$, $t_\\mathrm{max}$ and $t_\\mathrm{E}$. \n",
    "\n",
    "* $t_\\mathrm{E}$ is the Einstein crossing time - you can also get some initial guess for this from the data itself\n",
    "\n",
    "* $t_\\mathrm{max}$ is the time of maximum - which you should be able to read off from the plot\n",
    "\n",
    "* $p$ is the peak magnification - You can also get a guess of this and $I_0$ from the data. Look at the functional form of the model at times far from $t_\\mathrm{max}$.\n",
    "\n",
    "\n",
    "Lacking any better information, we'll assume that the sampling distributions for the magnitude measurements are Gaussian and independent, with means given by the \"magnitude\" column and standard deviations given by the \"magnitude error\" column, and that the time stamps are exact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's look at the data\n",
    "\n",
    "\n",
    "data = at.Table.read('phot.dat', format='ascii', names=['t','y','dy', 'seeing', 'sky'])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(data['t'], data['y'], yerr=data['dy'],\\\n",
    "            linestyle='None', marker='.', color='k', alpha=0.1)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('HJD')\n",
    "ax.set_ylabel('Mag')\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets declare a variable with the number of model dimensions\n",
    "ndim = 4\n",
    "\n",
    "\n",
    "def model(F_0, p, tmax, tE, t):   \n",
    "    u = np.sqrt(p**2 + ((t-tmax)/tE)**2) \n",
    "    F = F_0 *((u**2. + 2)/(u*np.sqrt(u**2 + 4)))\n",
    "    return F\n",
    "\n",
    "# before doing anything else, let's look at how this model behaves\n",
    "# tmax is an easy parameter, so lets see what happens when we change one of the others\n",
    "# holding the remaining fixed\n",
    "\n",
    "tarr = np.arange(-10, 11, 0.1)\n",
    "tmax = 0.\n",
    "tE = 1\n",
    "p = 1\n",
    "F_0 = 10.\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(12, 4))\n",
    "ax1, ax2, ax3 = axs\n",
    "\n",
    "for Fx in [5, 10, 15]:\n",
    "    ax1.plot(tarr, model(Fx, p, tmax, tE, tarr), label=f'F_0={Fx:.1f}')\n",
    "ax1.legend(frameon=False)\n",
    "ax1.set_xlabel('t')\n",
    "ax1.set_ylabel('F')\n",
    "ax1.set_title('Varying F_0')\n",
    "\n",
    "for px in [0.5, 1, 5]:\n",
    "    ax2.plot(tarr, model(F_0, px, tmax, tE, tarr), label=f'p={px:.1f}')\n",
    "ax2.legend(frameon=False)\n",
    "ax2.set_xlabel('t')\n",
    "ax2.set_ylabel('F')\n",
    "ax2.set_title('Varying p')\n",
    "\n",
    "for tx in [0.5, 1, 5]:\n",
    "    ax3.plot(tarr, model(F_0, p, tmax, tx, tarr), label=f'tE={tx:.1f}')\n",
    "ax3.legend(frameon=False)\n",
    "ax2.set_xlabel('t')\n",
    "ax3.set_ylabel('F')\n",
    "ax3.set_title('Varying tE')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since you didn't come up with the model (Bohdan Paczyinski did in 1986) this is a useful thing to do, if only to see how it behaves qualitatively.\n",
    "\n",
    "#### You can read more about microlensing here:\n",
    "http://www.astro.caltech.edu/~george/ay20/eaa-microlensing.pdf\n",
    "\n",
    "#### The only annoying thing here is that the flux gets higher numerically as the source gets lensed, but the data are in magnitudes which get lower as the source gets lensed and I suggested that it was simpler to just work in magnitudes. So -2.5 log10 the model and recognize that -2.5 log10(F_0) = I_0 and we should be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logmodel(I_0, p, tmax, tE, t):   \n",
    "    u = np.sqrt(p**2 + ((t-tmax)/tE)**2) \n",
    "    I = I_0 - 2.5*np.log10(((u**2. + 2)/(u*np.sqrt(u**2 + 4))))\n",
    "    return I\n",
    "\n",
    "tE = 1\n",
    "p = 1\n",
    "I_0 = 10.\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(12, 4))\n",
    "ax1, ax2, ax3 = axs\n",
    "\n",
    "for Ix in [5, 10, 15]:\n",
    "    ax1.plot(tarr, logmodel(Ix, p, tmax, tE, tarr), label=f'I_0={Ix:.1f}')\n",
    "ax1.legend(frameon=False)\n",
    "ax1.set_xlabel('t')\n",
    "ax1.set_ylabel('F')\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title('Varying F_0')\n",
    "\n",
    "for px in [0.5, 1, 5]:\n",
    "    ax2.plot(tarr, logmodel(I_0, px, tmax, tE, tarr), label=f'p={px:.1f}')\n",
    "ax2.legend(frameon=False)\n",
    "ax2.set_xlabel('t')\n",
    "ax2.set_ylabel('F')\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_title('Varying p')\n",
    "\n",
    "for tx in [0.5, 1, 5]:\n",
    "    ax3.plot(tarr, logmodel(I_0, p, tmax, tx, tarr), label=f'tE={tx:.1f}')\n",
    "ax3.legend(frameon=False)\n",
    "ax2.set_xlabel('t')\n",
    "ax3.set_ylabel('F')\n",
    "ax3.set_title('Varying tE')\n",
    "ax3.invert_yaxis()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, so now we need a reasonable initial guess for Emcee\n",
    "\n",
    "\n",
    "### I_0 is roughly the baseline level of the flux which for my real light curve looks like 17.25 ish\n",
    "\n",
    "### p is something like a peak magnification. p of 0.5 seems to cause a 0.8 mag change, while p of 1 seems to cause a 0.25 mag change. Ours is somewhere in between so we'll try p=0.75\n",
    "\n",
    "### tE is roughy the half-width at half-maximum. Our event seems to last roughly a 100 days FWHM, so we'll take half that as our initial guess.\n",
    "\n",
    "### Finally, our peak looks somewhere around HJD=2458500.\n",
    "\n",
    "### You can do way better with using some simple stats that you've already learned about this semester, but I'm deliberately winging it by doing this by eye... because you should *LOOK AT YOUR DATA!*\n",
    "\n",
    "### Let's just look at those guesses vs the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(data['t'], data['y'], yerr=data['dy'],\\\n",
    "            linestyle='None', marker='.', color='k', alpha=0.1)\n",
    "ax.invert_yaxis()\n",
    "tmin, tmax = ax.get_xlim()\n",
    "tarr = np.arange(tmin, tmax+0.1, 0.1)\n",
    "guess = [17.25, 0.75, 2458500, 50]\n",
    "ax.plot(tarr, logmodel(*guess, tarr), ls=':', color='C0', label='Wild guess by eye')\n",
    "ax.set_xlabel('HJD')\n",
    "ax.set_ylabel('Mag')\n",
    "ax.legend(frameon=False)\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should be obvious here that the errors are underestimated - look at the scatter\n",
    "# of the points vs the size of the error bars\n",
    "# we can use the measurements in the first couple of years to get a sense for\n",
    "# how badly underestimated it is\n",
    "\n",
    "indt = (data['t'] > data['t'].min()) & (data['t'] < data['t'].min()+365*2.)\n",
    "\n",
    "# we can compare the variance of the measurements with the noise\n",
    "print(\"Stdev of measurements:\" , np.std(data['y'][indt]))\n",
    "print(\"Median measurement error:\", np.median(data['dy'][indt]))\n",
    "unoise = np.sqrt(np.std(data['y'][indt]) **2. - np.median(data['dy'][indt])**2.)\n",
    "print(\"Underestimated noise:\", unoise)\n",
    "# this is a high-ish value so we'll add it to the denominator of our chi-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You could refine your guess as well with e.g. scipy.optimize. You already know how to do this with something simple like maximum likelihood! You also know how to get an initial estimate for reasonable step sizes in each direction - you'd like to step by roughly some factor of the variance of each of the parameters. This is just the Hessian Inverse/Fisher Information, which scipy.optimize will kindly give us.\n",
    "\n",
    "## Hint: If you are stuck on the homework with emcee, try scipy.optimize first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll define a (reduced) chisq function for convenience\n",
    "def chisq(x, *args):\n",
    "    I_0, p, tmax, tE = x\n",
    "    t, y, dy = args\n",
    "    mod = logmodel(I_0, p, tmax, tE, t)\n",
    "    chisq = np.sum(((y - mod)**2.)/(dy**2. + unoise**2.))/(len(t) -4)\n",
    "    return chisq \n",
    "    \n",
    "# and a log likelihood function for Emcee\n",
    "# note that Emcee really does want the actual honest to goodness likelihood\n",
    "# not reduced chi-squared, so I multiply by the number of degrees of freedom\n",
    "# otherwise the likelihood and prior would not have the right\n",
    "# relative normalization\n",
    "def LogLikelihood(I_0, p, tmax, tE, t, y, dy):\n",
    "    x = (I_0, p, tmax, tE)\n",
    "    args = (t, y, dy)\n",
    "    negLogLike = chisq(x, *args)*(len(t) -4)\n",
    "    return -negLogLike/2.\n",
    "\n",
    "# we'll set some very loose bounds on each parameter\n",
    "bounds = [(15, 18), (0, 3), (2458400, 2458600), (10, 70)]\n",
    "\n",
    "# we need to pass the data to scipy.optimize\n",
    "args = (data['t'], data['y'], data['dy'])\n",
    "\n",
    "# if you wnat to see how to use scipy.optimize, here's an example \n",
    "# uncomment lines with a #####\n",
    "##### res = so.minimize(chisq, guess, args=args, bounds=bounds)\n",
    "##### print(res)\n",
    "\n",
    "\n",
    "# without scipy, I need some scale to start the walkers - ones will do...\n",
    "sigmas = np.ones(4)\n",
    "\n",
    "# if you use scipy.optimize you can use the Hessian inverse \n",
    "# to set the position for emcee's walkers\n",
    "# this choice is good because it should ensure we sample the distribution well.\n",
    "##### sigmas = np.diag(res.hess_inv.todense())**0.5\n",
    "##### print('Sigmas =', sigmas)\n",
    "# there's a line further on with a plot of the result with scipy if you run this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we need a prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogPrior(I_0, p, tmax, tE):\n",
    "    \n",
    "    # we can put some big bounds on the parameters\n",
    "    if (I_0 >= 18  or I_0 <= 15) or\\\n",
    "        (p <= 0 or p >= 3)  or\\\n",
    "        (tE <= 10 or tE > 70) or\\\n",
    "        (tmax <= 2458400 or tmax >= 2458600):   \n",
    "        return -np.inf\n",
    "    \n",
    "    # we can also make the priors non-trivial\n",
    "    # but we also know we want the priors to be somewhat uninformative\n",
    "    # the data should determine the model parameters, not the prior\n",
    "    # so I'll make the widths very wide\n",
    "    p_I0 = st.norm.pdf(I_0, loc=1.72653507e+01, scale=1.)\n",
    "    p_p = st.norm.pdf(p, loc=6.87406601e-01, scale=1.)\n",
    "    p_tmax = st.norm.pdf(tmax, loc=2.45852788e+06, scale=10.)\n",
    "    p_tE = st.norm.pdf(tE, loc=2.54557543e+01, scale=10.)\n",
    "    prior = p_I0*p_p*p_tmax*p_tE\n",
    "    \n",
    "    lnprior = np.log(prior)\n",
    "    return lnprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we need the posterior - or really the log of the posterior for our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogPosterior(params, t=None, y=None, dy=None):\n",
    "    I_0, p, tmax, tE = params\n",
    "    lnlike = LogLikelihood(I_0, p, tmax, tE, t, y, dy)\n",
    "    lnprior = LogPrior(I_0, p, tmax, tE)\n",
    "    lnposterior = lnlike + lnprior\n",
    "    return lnposterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And that's everything we need - I'll initalize the walkers with my guess. If you ran scipy.optimize, you could start with that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers = 20\n",
    "kwargs = {'t':data['t'], 'y':data['y'], 'dy':data['dy']}\n",
    "\n",
    "# we'll initialize the walkers in a small ball around our best fit position - they'll meander away in the next step\n",
    "p0 = np.array(guess) + 0.1*sigmas*np.random.randn(nwalkers, ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, LogPosterior, kwargs=kwargs )\n",
    "state = sampler.run_mcmc(p0, 1000) # do a little burn-in - note that this may not be enough - you should check \n",
    "sampler.reset()                    # your traceplot at the end to decide - but it's a way to start the walkers off \n",
    "                                   # in more random locations - we'll discard these 1000 steps entirely\n",
    "                                    \n",
    "# run the chain in production\n",
    "# I'll do 10,000 steps. This is complete overkill.\n",
    "sampler.run_mcmc(state, 10000, progress=True);\n",
    "\n",
    "# we've asked for 10,000 steps x 20 walkers, so our chain should be [10000, 20, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain(flat=True) # you can flatten the chain - essentially concatenate the walkers\n",
    "samples.shape                          # so now you've got 20 * 10000, 4 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visualize the chains\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "labels=['I_0', 'p', 'tmax', 'tE']\n",
    "for i in range(samples.shape[1]):\n",
    "    j, k = np.unravel_index(i, (2,2))\n",
    "    ax = axs[j][k]\n",
    "    ax.plot(samples[::10,i], linestyle='-', color='grey', marker='.', alpha=0.1)\n",
    "    med = np.median(samples[:,i])\n",
    "    std = np.std(samples[:,i])\n",
    "    ax.set_ylim(med-3*std, med+3*std) \n",
    "    ax.set_ylabel(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's hard to see what the burn-in is here because my initial guess by eye wasn't terrible. Let's discard 10% at the start as burnin just to be conservative. We'll also take every 10th sample to reduce the correlation between samples on the chains (see lecture 6 for thinning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(samples[1000::10,:], plot_contours=True, show_titles=True, smooth=2, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have all the samples so you can ask for the 16 and 84th percentile \n",
    "# to set uncertainties\n",
    "mcmc_res = np.percentile(samples[1000::10,:], [50, 16, 84], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the data \n",
    "ax.errorbar(data['t'], data['y'], yerr=data['dy'],\\\n",
    "            linestyle='None', marker='.', color='k', alpha=0.1)\n",
    "ax.invert_yaxis() \n",
    "tmin, tmax = ax.get_xlim()\n",
    "\n",
    "# compare against our original guess\n",
    "tarr = np.arange(tmin, tmax+0.1, 0.1)\n",
    "guess = [17.25, 0.75, 2458500, 50]\n",
    "ax.plot(tarr, logmodel(*guess, tarr), ls=':', color='C0', label='Wild guess by eye')\n",
    "\n",
    "\n",
    "# you can use the MCMC 16th and 84th percentile to make an uncertainty region\n",
    "ax.fill_between(tarr, logmodel(*mcmc_res[1], tarr), logmodel(*mcmc_res[2], tarr),\\\n",
    "                    color='C2', alpha=0.3)\n",
    "# and report the median value as the \"Result\"\n",
    "ax.plot(tarr, logmodel(*mcmc_res[0], tarr), ls='-', color='C1', label='MCMC Result')\n",
    "\n",
    "\n",
    "# we can also compare the detailed fit\n",
    "axins = ax.inset_axes([0.1, 0.5, 0.47, 0.47])\n",
    "axins.errorbar(data['t'], data['y'], yerr=data['dy'],\\\n",
    "            linestyle='None', marker='.', color='k', alpha=0.1)\n",
    "axins.plot(tarr, logmodel(*guess, tarr), ls=':', color='C0')\n",
    "axins.plot(tarr, logmodel(*mcmc_res[0], tarr), ls='-', color='C1')\n",
    "axins.set_xlim(mcmc_res[0][2] - 30., mcmc_res[0][2] + 30. )\n",
    "axins.set_ylim(17.1, 16.6)\n",
    "\n",
    "\n",
    "# if you ran with scipy.optimize, you can uncomment this \n",
    "##### ax.plot(tarr, logmodel(*res.x, tarr), ls='--', color='C2',\\\n",
    "#####          label='Refined with a local optimizer')\n",
    "\n",
    "\n",
    "ax.set_xlabel('HJD')\n",
    "ax.set_ylabel('Mag')\n",
    "ax.legend(frameon=False)\n",
    "fig.autofmt_xdate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:astr496] *",
   "language": "python",
   "name": "conda-env-astr496-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
